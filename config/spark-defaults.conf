spark.hadoop.fs.s3a.access.key=minio
spark.hadoop.fs.s3a.secret.key=minio123
spark.hadoop.fs.s3a.endpoint=http://minio:9000
spark.hadoop.fs.s3a.path.style.access=true
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem

spark.master                     spark://master:7077
spark.ui.enabled true

#----------------------------------------------#

# Spark driver configuration
# spark.driver.memory 1g
# spark.driver.cores  1

# # Spark executor configuration
# spark.executor.instances 2
# spark.executor.cores 1
# spark.executor.memory 1g

# Spark dynamic allocation (optional, for resource efficiency)
# spark.dynamicAllocation.enabled true

#---------------------------------------------#
# spark.eventLog.enabled           true

# Use This To Save logs in s3 Bucket
# spark.eventLog.dir               s3a://spark-logs/logs

# OR USE This to save logs locally But Uncomment the env and volume related for logs in docker-compose:
# spark.eventLog.dir             file:/opt/bitnami/spark/logs
#---------------------------------------------#

# Set log directory for the Spark History Server
# spark.history.fs.logDirectory file:/opt/spark/logs

